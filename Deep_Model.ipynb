{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoussefDiaa1/Project-ITI/blob/main/Deep_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYf6ZGiKCoWL",
        "outputId": "222e053f-ee2a-486b-d1a6-c4167245c954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SWJG0iTMhdU",
        "outputId": "e532479e-683c-4e80-ffc0-768395c0c07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now working inside: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# اختار مسار آمن\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "print(\"Now working inside:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4KV2kVlMrR2",
        "outputId": "2616f4c8-7362-4f4b-9853-b087c39979e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created: /content/prepared_data\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR = \"prepared_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Created:\", os.path.abspath(OUTPUT_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVuX7efDMY8n"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/prepared_data\n",
        "OUTPUT_DIR = \"/content/prepared_data\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9qZvgziDOzg",
        "outputId": "4b5a1c87-dc02-40b1-f32d-0d81bcc7eead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged dataset size: 81078\n",
            "Languages distribution: {'en': 50000, 'ar': 31078}\n",
            "Label mapping: {'0': 0, '1': 1, 'negative': 2, 'positive': 3}\n",
            "Label distribution: {2: 25000, 3: 25000, 1: 18406, 0: 12672}\n",
            "After oversampling label distribution: {3: 25000, 0: 25000, 2: 25000, 1: 25000}\n",
            "Final sizes -> train: 80000 val: 10000 test: 10000\n",
            "Train label dist: {0: 20000, 2: 20000, 3: 20000, 1: 20000}\n",
            "Saved: ./prepared_data/train.csv\n",
            "Saved: ./prepared_data/validation.csv\n",
            "Saved: ./prepared_data/test.csv\n",
            "Saved: ./prepared_data/merged_all.csv\n",
            "Done. Files in: ./prepared_data\n"
          ]
        }
      ],
      "source": [
        "# file: prepare_multilang_reviews.py\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# ---------- إعدادات ----------\n",
        "AR_FILE = \"/content/drive/MyDrive/Review_Classification_Data_AR.csv\"\n",
        "EN_FILE = \"/content/drive/MyDrive/Review_Classification_Data_EN.csv\"\n",
        "OUTPUT_DIR = \"prepared_data\"\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.1   # نسبة الاختبار النهائية\n",
        "VAL_SIZE = 0.1    # نسبة الفاليدشن من الباقي بعد استقطاع الاختبار\n",
        "OVERSAMPLE = True  # لو True سيقوم بزيادة عينات الفئات الصغيرة\n",
        "OUTPUT_DIR = \"./prepared_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- دوال مساعدة للتنظيف ----------\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "def remove_html(text):\n",
        "    return re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "def remove_control_chars(text):\n",
        "    return ''.join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Arabic-specific normalization (light)\n",
        "AR_DIACRITICS = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u06D6-\\u06ED]')\n",
        "def normalize_arabic(text):\n",
        "    text = remove_urls(text)\n",
        "    text = remove_html(text)\n",
        "    text = remove_control_chars(text)\n",
        "    text = AR_DIACRITICS.sub('', text)            # remove tashkeel\n",
        "    text = re.sub(r'[\\u0640]', '', text)          # remove tatweel\n",
        "    # normalize Alef variants\n",
        "    text = re.sub(r'[إأآﺇﺃآ]', 'ا', text)\n",
        "    # normalize Ya and Alef Maqsura\n",
        "    text = re.sub(r'[ى]', 'ي', text)\n",
        "    # hamza forms to bare hamza\n",
        "    text = re.sub(r'[ؤئ]', 'ء', text)\n",
        "    # optionally keep ة as is (tāʾ marbuta) — we keep it\n",
        "    # remove extra punctuation except basic\n",
        "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
        "    return normalize_whitespace(text)\n",
        "\n",
        "# English-specific normalization (light)\n",
        "def normalize_english(text):\n",
        "    text = remove_urls(text)\n",
        "    text = remove_html(text)\n",
        "    text = remove_control_chars(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^0-9a-zA-Z\\s\\']', ' ', text)  # keep apostrophes\n",
        "    return normalize_whitespace(text)\n",
        "\n",
        "# generic cleaner that calls language-specific normalizers\n",
        "def clean_text(text, lang):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text) if pd.notna(text) else \"\"\n",
        "    text = text.strip()\n",
        "    if lang == \"ar\":\n",
        "        return normalize_arabic(text)\n",
        "    else:\n",
        "        return normalize_english(text)\n",
        "\n",
        "# ---------- محاولة اكتشاف أعمدة النص والـ label تلقائيًا ----------\n",
        "def pick_text_label_columns(df):\n",
        "    text_candidates = [\"text\", \"review\", \"content\", \"sentence\", \"comment\", \"body\"]\n",
        "    label_candidates = [\"label\", \"labels\", \"sentiment\", \"rating\", \"target\", \"class\"]\n",
        "    text_col = None\n",
        "    label_col = None\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "\n",
        "    # find text col\n",
        "    for cand in text_candidates:\n",
        "        for c in df.columns:\n",
        "            if c.lower() == cand:\n",
        "                text_col = c\n",
        "                break\n",
        "        if text_col: break\n",
        "\n",
        "    # fallback: any object/string column\n",
        "    if text_col is None:\n",
        "        for c in df.columns:\n",
        "            if df[c].dtype == object:\n",
        "                text_col = c\n",
        "                break\n",
        "\n",
        "    # find label col\n",
        "    for cand in label_candidates:\n",
        "        for c in df.columns:\n",
        "            if c.lower() == cand:\n",
        "                label_col = c\n",
        "                break\n",
        "        if label_col: break\n",
        "\n",
        "    # fallback: any numeric or categorical column with few unique values\n",
        "    if label_col is None:\n",
        "        for c in df.columns:\n",
        "            if df[c].dtype in [int, float] or df[c].dtype == object:\n",
        "                if df[c].nunique() <= 20 and df[c].nunique() > 1:\n",
        "                    label_col = c\n",
        "                    break\n",
        "\n",
        "    return text_col, label_col\n",
        "\n",
        "# ---------- قراءة الملفات وتوحيد الأعمدة ----------\n",
        "def read_and_prepare(path, lang_tag):\n",
        "    df = pd.read_csv(path)\n",
        "    text_col, label_col = pick_text_label_columns(df)\n",
        "    if text_col is None or label_col is None:\n",
        "        raise ValueError(f\"Couldn't auto-detect text/label columns in {path}. Cols: {df.columns.tolist()}\")\n",
        "    df = df[[text_col, label_col]].rename(columns={text_col: \"text\", label_col: \"label\"})\n",
        "    df[\"lang\"] = lang_tag\n",
        "    # clean\n",
        "    df[\"text\"] = df[\"text\"].apply(lambda t: clean_text(t, lang_tag))\n",
        "    # drop empty texts\n",
        "    df = df[df[\"text\"].str.strip() != \"\"]\n",
        "    # reset index\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# ---------- دمج الملفين ----------\n",
        "df_ar = read_and_prepare(AR_FILE, \"ar\")\n",
        "df_en = read_and_prepare(EN_FILE, \"en\")\n",
        "df = pd.concat([df_ar, df_en], ignore_index=True)\n",
        "print(\"Merged dataset size:\", len(df))\n",
        "print(\"Languages distribution:\", df[\"lang\"].value_counts().to_dict())\n",
        "\n",
        "# ---------- تأكد من أن الـ labels هي أرقام 0..K-1 (map إذا كانت نصية) ----------\n",
        "df[\"label\"] = df[\"label\"].astype(str).str.strip()  # نخليها كلها نصوص\n",
        "\n",
        "label_values = sorted(df[\"label\"].unique().tolist())\n",
        "label_map = {lab: i for i, lab in enumerate(label_values)}\n",
        "\n",
        "df[\"label\"] = df[\"label\"].map(label_map)  # حول النصوص لأرقام\n",
        "\n",
        "print(\"Label mapping:\", label_map)\n",
        "print(\"Label distribution:\", df[\"label\"].value_counts().to_dict())\n",
        "\n",
        "# حفظ الـ mapping\n",
        "with open(os.path.join(OUTPUT_DIR, \"label_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------- حل مشكلة عدم التوازن (اختياري - oversample) ----------\n",
        "if OVERSAMPLE:\n",
        "    counts = df[\"label\"].value_counts()\n",
        "    max_count = counts.max()\n",
        "    dfs = []\n",
        "    for label_val, group in df.groupby(\"label\"):\n",
        "        if len(group) < max_count:\n",
        "            upsampled = resample(group, replace=True, n_samples=max_count, random_state=RANDOM_STATE)\n",
        "            dfs.append(upsampled)\n",
        "        else:\n",
        "            dfs.append(group)\n",
        "    df = pd.concat(dfs).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "    print(\"After oversampling label distribution:\", df[\"label\"].value_counts().to_dict())\n",
        "\n",
        "# ---------- تقسيم Stratified إلى train / val / test ----------\n",
        "# أولاً فصل الاختبار النهائي\n",
        "trainval_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df[\"label\"])\n",
        "# ثم من الباقي نأخذ validation\n",
        "val_relative_size = VAL_SIZE / (1.0 - TEST_SIZE)\n",
        "train_df, val_df = train_test_split(trainval_df, test_size=val_relative_size, random_state=RANDOM_STATE, stratify=trainval_df[\"label\"])\n",
        "\n",
        "print(\"Final sizes -> train:\", len(train_df), \"val:\", len(val_df), \"test:\", len(test_df))\n",
        "print(\"Train label dist:\", train_df[\"label\"].value_counts().to_dict())\n",
        "\n",
        "# ---------- حفظ CSVs جاهزة للتدريب (text,label,lang) ----------\n",
        "for name, d in [(\"train.csv\", train_df), (\"validation.csv\", val_df), (\"test.csv\", test_df), (\"merged_all.csv\", df)]:\n",
        "    out_path = os.path.join(OUTPUT_DIR, name)\n",
        "    d[[\"text\", \"label\", \"lang\"]].to_csv(out_path, index=False)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "print(\"Done. Files in:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-9SPI8HxcKT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaHoAi8UWv-J"
      },
      "outputs": [],
      "source": [
        "# file: train_multilang_reviews.py\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Bs7R_bWx1R"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# إعداد المسارات والمتغيرات\n",
        "# -----------------------------\n",
        "DATA_DIR = \"./prepared_data\"       # المكان اللي فيه CSVs\n",
        "MODEL_NAME = \"xlm-roberta-base\"    # أو \"xlm-roberta-large\" لو GPU كبير\n",
        "OUTPUT_DIR = \"./multilang_model\"\n",
        "NUM_LABELS = 4                     # حسب الـ label mapping اللي عندك\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdBt4yRCWzjZ"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# تحميل البيانات من CSV\n",
        "# -----------------------------\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "val_df   = pd.read_csv(os.path.join(DATA_DIR, \"validation.csv\"))\n",
        "test_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "2c75c5c704d343cdabf1b2e3e5bf6fb4",
            "ed49c943bf9541f0a97d7972d516bc21",
            "ebe0e7c151044f78baed646d7f30a0a5",
            "8c402efc86cd491cb38b11dcf04d6fe8",
            "325c081e966245eea4364267135ecd82",
            "6dced226378e40e884813b2d1ef98984",
            "a1ba170d0a744f8dbb840dec6b93804b",
            "13047819bdcd43fbb8e2dd0fe94fec87",
            "5ac9fcfe40c94f4c9e39ddba2cef2f7e",
            "fc7399a7a5a740b392ccc674c067a5ea",
            "4c7d5c546c7c4e35982a6375c70d4236",
            "200077fe3c36448b823e5bc57338265b",
            "8e66f00c28a945b6a98a2fb502e3db9e",
            "b3e4626a16094427930955f7e006e067",
            "44b1e890233343f9b14c6b8bd0d20b5a",
            "5334ea98eeae4272bf16ced38e1a44eb",
            "f75cf4c8c5d74353968abb3aa15e9e9a",
            "d05ad5a917a74e0e9c7585dc14367671",
            "9e5fad514c614351937d8d1ccc22a703",
            "6eb65f0950f345459c8e8a9489447361",
            "a1cab00c9cfb4b09932c033c59966d4f",
            "c6c2a000aea244bf8713ebf7b066433f",
            "603f880144ff41f0a03b9d5f82b43cfd",
            "94adebe3d1a8489686a5c1044af16224",
            "2d749369085b40989f644e90bebab9f1",
            "66db207fbef448788809ae465bfc9439",
            "1d9893c93c474f9489f65d8ed655ec75",
            "d4a9ac3abade48d1b6e680526cdd3443",
            "96052091354e4e348f74e823bac04dbd",
            "f11f941db3e24959bc75d78c46bb045a",
            "16558529e9074e4a8863264b09bf4347",
            "1250a4cbb8a2411789bd95741e6be7d9",
            "13782ce71af84eb090ed84537268be0e",
            "f324a89fb29f4fa5afdcc5d9c752ed05",
            "f35e685a7bd24bac8bdb0f39dc6ec9e0",
            "f4d00cb6be2a46629bb44f58b7cf35ff",
            "395677318ba74e2c80b192865282d15d",
            "7fd73b640b18469c90e3644dbfc0f868",
            "b902c7dfa5ae4975816aa682b2d2b5b0",
            "abcc7963419b45039433bbda894b7898",
            "bb7ca0217efc49ae9c0b1b6a9e52e870",
            "568be48a001642ada4e0f736b9730bb7",
            "f7b5cb13cc394c5884129fa8ec64d1ac",
            "65b05aa4a53e4caf8b1864b2a8c1db1f"
          ]
        },
        "id": "zmflrYzOW1rs",
        "outputId": "f6278580-b8d7-45b5-af07-3d3c92d1e897"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c75c5c704d343cdabf1b2e3e5bf6fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "200077fe3c36448b823e5bc57338265b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "603f880144ff41f0a03b9d5f82b43cfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f324a89fb29f4fa5afdcc5d9c752ed05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Tokenizer\n",
        "# -----------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"].tolist(),\n",
        "                     padding=\"max_length\",\n",
        "                     truncation=True,\n",
        "                     max_length=MAX_LEN,\n",
        "                     return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgIDIbJWW33u"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Dataset class لـ Hugging Face\n",
        "# -----------------------------\n",
        "class ReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[\"label\"].tolist()\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx],\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             max_length=MAX_LEN,\n",
        "                             return_tensors=\"pt\")\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "train_dataset = ReviewsDataset(train_df)\n",
        "val_dataset   = ReviewsDataset(val_df)\n",
        "test_dataset  = ReviewsDataset(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOhhKUJqW5RG"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Metrics function\n",
        "# -----------------------------\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"precision_macro\": precision_score(labels, preds, average=\"macro\"),\n",
        "        \"recall_macro\": recall_score(labels, preds, average=\"macro\")\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "8494599d90504e21a174b30479aae1e5",
            "e71e4c198edd454cb3020237458a3b20",
            "5b5c651d3cfc414297c54487bd0cb5ef",
            "726a6fd5a2d14fcdb858188ccc4dca73",
            "a36b75af90ad4a688a5a542317c47fa5",
            "8b96c775329b43b987fd0f8384c8c5f8",
            "1c526537f5f64260968f7f441cfc3521",
            "1260d1748fb74c0c8d96066b5abb1101",
            "7c772f1143dc40cf8efdbbacae36872c",
            "13de905967a343f6adc9973d15959978",
            "e27c11a9e8fb4bfebf6a17738dc140d3"
          ]
        },
        "id": "H9O9XSTPW7xH",
        "outputId": "9c0ced0a-67dd-418f-c5e3-e68fb55b27b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8494599d90504e21a174b30479aae1e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# تحميل موديل للتصنيف\n",
        "# -----------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "104nClodW7p_"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# TrainingArguments\n",
        "# -----------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMxONQ7-W-8B"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Trainer\n",
        "# -----------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "JFCjnIg3XAD0",
        "outputId": "6b5cabfc-0aa0-4814-b280-fdfd82476f28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamrhassanhd\u001b[0m (\u001b[33mamrhassanhd-ieee\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250826_220225-ty5pz2kr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/amrhassanhd-ieee/huggingface/runs/ty5pz2kr' target=\"_blank\">curious-disco-2</a></strong> to <a href='https://wandb.ai/amrhassanhd-ieee/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/amrhassanhd-ieee/huggingface' target=\"_blank\">https://wandb.ai/amrhassanhd-ieee/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/amrhassanhd-ieee/huggingface/runs/ty5pz2kr' target=\"_blank\">https://wandb.ai/amrhassanhd-ieee/huggingface/runs/ty5pz2kr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15000/15000 1:10:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.316100</td>\n",
              "      <td>0.303574</td>\n",
              "      <td>0.877000</td>\n",
              "      <td>0.876898</td>\n",
              "      <td>0.878283</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.266400</td>\n",
              "      <td>0.286916</td>\n",
              "      <td>0.896000</td>\n",
              "      <td>0.895989</td>\n",
              "      <td>0.896185</td>\n",
              "      <td>0.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.208200</td>\n",
              "      <td>0.312626</td>\n",
              "      <td>0.901400</td>\n",
              "      <td>0.901383</td>\n",
              "      <td>0.901656</td>\n",
              "      <td>0.901400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15000, training_loss=0.2893467235565186, metrics={'train_runtime': 4253.1296, 'train_samples_per_second': 56.429, 'train_steps_per_second': 3.527, 'total_flos': 3.157389361152e+16, 'train_loss': 0.2893467235565186, 'epoch': 3.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# تدريب الموديل\n",
        "# -----------------------------\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ezlVaKUnXBq9",
        "outputId": "0f4b0151-e010-4f9f-a93c-7251d77944ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 54:07]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set results: {'eval_loss': 0.3137753903865814, 'eval_accuracy': 0.901, 'eval_f1_macro': 0.9009891693486665, 'eval_precision_macro': 0.9011521077161999, 'eval_recall_macro': 0.901, 'eval_runtime': 42.1546, 'eval_samples_per_second': 237.222, 'eval_steps_per_second': 14.826, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# تقييم على Test set\n",
        "# -----------------------------\n",
        "results = trainer.evaluate(test_dataset)\n",
        "print(\"Test set results:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrJ2YxBPXC3x",
        "outputId": "87ae94fe-8cd4-44f1-fd1a-ba2188a5e391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer saved to ./multilang_model\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# حفظ الموديل النهائي\n",
        "# -----------------------------\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZi1sGXY4o8G",
        "outputId": "49685818-e513-491c-f0a4-fea7fcb684c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created ZIP file: ./multilang_model.zip\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "OUTPUT_DIR = \"./multilang_model\"\n",
        "ZIP_PATH = OUTPUT_DIR + \".zip\"\n",
        "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n",
        "\n",
        "print(\"Created ZIP file:\", ZIP_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259,
          "referenced_widgets": [
            "83a20a79553041b5a2c9f99b3cf0c6d5",
            "6ada54475f7648cbbd0c59676b8ddd49",
            "aae5205174014e9588fb56506cdc70a6",
            "115c14db678b4616b6a2558c8430af3f",
            "1982a0919713400d87fec6b9774f08f8",
            "715401a6a8ae4089aa4618a2bbde89c7",
            "072c5d9a07d143958b4ab5d60cea2fff",
            "5bb542f2797840619d3a602c6001dee0",
            "4986ac809c9c42e59fca1ae482b25dd0",
            "ae75268626fe47a1805c1a65d0c7d20b",
            "5f9cee3aacde404b893348c26d067e36",
            "ed4882532237401f853a33af2f549c20",
            "44e0c3c7fd504c6598529ce3f75f5f4e",
            "d1a844412f9f467db2232c584f6f0af2",
            "72c587529af4448b831e8b219f066591",
            "36221c6ae8104386902eb20eea09307a",
            "c01e331e7bfb4fa6ad0cfbba1eccc371",
            "02a0fd0cb6384b3e9fd2db2b0f06137b",
            "facc025230e6437499deeee9eae3a48e",
            "ef1d847f162a48ec970056070363ff92",
            "7c20c6112e0a4ce4a8be9076e9528853",
            "7f4b9308ae5844efb1e6e3cb64020c1f",
            "dd0c24b8e2e2464e8652c53268949702",
            "3cd1c37b9fcf46b28bbfbe915dc1f64e",
            "44c9fb862ecf4f5f935bc9d37eb51b80",
            "d4566e7ccef448c085df3f404b2d42d2",
            "f88e84d28f944db6827a968e760767bb",
            "5edbb448ea4a4aada65d8a51d8a47dde",
            "60cfe8748c1c41d7a843161d51a16354",
            "85e9c87cd9c84f48ac8d0ca517f5b236",
            "31009953c1aa412e9b6806bfb9d93e35",
            "02d2d25c4b7e46a28b8a2c8406a55606",
            "cd603373e8df485ea99b536f8eae04ee",
            "014ca1f9ec1148119f811b386f924d86",
            "d08b745914474e08bc5699ab3ed5f63e",
            "0eefb28a2f59453ba9a9fa40010914e1",
            "00a03fb7c50c4e8880a42b99be6b4e07",
            "edfa1b5fd112428eb0fe0b28514dd9b1",
            "1b92457ff9ee468799e250500b2c734b",
            "4d7ec271854543ee80046a576402f1b1",
            "b262cde72dfc4d769280b7b2305a78b1",
            "865b7cf94b09470192ed756dc5f91be1",
            "6a71393899a34068a8aee1b5303a9277",
            "9aa1fb8ba1904330b36cf1b94edd9b9b",
            "cdb9f6ea160e445ca25c93885c157bc8",
            "0377f4f53639470fa3f9952d183520b0",
            "41ee17ce093d4b0dbbd7cf44fad64d82",
            "cd41276ce61e4a90ab128748f96c9dfa",
            "8118e26296834ee2a99b48aedc4edf1d",
            "facc8028a63d4c1abe47617897090d34",
            "c00d1a2fc04f4763b32b78a3eb3028a1",
            "cbad07f539994c0f80d862607757bc54",
            "6660a216075947f4ac4a94624e53a923",
            "053dcba48c154bfa8a46fe5b4b5ef800",
            "5b02d082a0b140629e722d07eceff232",
            "c3b94d38083845e783c867d6e2543dc5",
            "594d71662f54407f875120fa50ebbc1d",
            "188a626a63594f83aa86c0ad3c3070a5",
            "d5eff51a06e54b699a015ce6b550736c",
            "152eb83b881c4fef993774f1dd14598a",
            "934b6f64f148401d98ca1eaf4a764635",
            "4f5bd52a7b54451a91f95b98aa6738fb",
            "264e5fafea804d868cb5ccfb6d58eced",
            "ae07f0fb272440969a55b798e099bd9c",
            "88706a85fa494af180de2603b609f493",
            "155e3fd68d9d4420b654307b828acfa5",
            "6400c3947d674dd5ac34d0b9c32b00b4",
            "51e9390c4e174fcbbe8af44648cdd3e1",
            "13d449960b2940168903f92e0a5ef3ac",
            "cd3db21e60b640b5910981db5a739bd8",
            "9af8bed5d9824a51bbcaa81f93c9f656",
            "71c90c7bca9a48599fbb3db7f5cc6447",
            "4d1c38e57cc848c39853b445cba4d9e9",
            "2ddbcc7c8cc94dc9832aa88d4696a2e9",
            "94bebcfc3967414caea69a1c3fab2d58",
            "b1b46b632c6947b9a59fbdfdd20f90b9",
            "23637c4551a74185b98c3bd2300cab56",
            "a987f3d81f004721863c495aef996ff1",
            "70eaea85610541bb80fee6208f441fa6",
            "81ac84840b0c4f25ad460a386ee428be",
            "c747a5a90f284afebc22b869380f3b79",
            "adef73b5727a499b99e5352531fb444b",
            "721252bfe4f9498f9b30434667c61daa",
            "0ec9e74bbfd64663ada70f20b131ec2d",
            "8e265d690dda45d0866e0b08ca5e7e4e",
            "1a14fb4a3b1442a4b2736868cf7a5152",
            "48fef3c569064a70912ef70da4d38864",
            "eba8b85078274be9b830819830534d69",
            "8e7187bb516d4c6592c45e017f632412"
          ]
        },
        "id": "8XZs7GMB6sFG",
        "outputId": "1d5fe7e5-ccfd-4aaf-ea44-92e73c77685f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83a20a79553041b5a2c9f99b3cf0c6d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02a0fd0cb6384b3e9fd2db2b0f06137b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60cfe8748c1c41d7a843161d51a16354",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7ec271854543ee80046a576402f1b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...tent/multilang_model/tokenizer.json: 100%|##########| 17.1MB / 17.1MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c00d1a2fc04f4763b32b78a3eb3028a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...t/multilang_model/model.safetensors:   0%|          | 1.12MB / 1.11GB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f5bd52a7b54451a91f95b98aa6738fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...t/multilang_model/training_args.bin: 100%|##########| 5.71kB / 5.71kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d1c38e57cc848c39853b445cba4d9e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...ilang_model/sentencepiece.bpe.model: 100%|##########| 5.07MB / 5.07MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model uploaded successfully to: https://huggingface.co/amrhassank/Review_Classification\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, upload_folder, create_repo\n",
        "\n",
        "# تسجيل الدخول\n",
        "from huggingface_hub import login\n",
        "login()  # حط الـ token هنا\n",
        "\n",
        "# اسم الريبو\n",
        "REPO_ID = \"amrhassank/Review_Classification\"\n",
        "\n",
        "# إنشاء الريبو لو مش موجود\n",
        "create_repo(repo_id=REPO_ID, exist_ok=True)  # exist_ok=True مش هيعمل مشكلة لو الريبو موجود بالفعل\n",
        "\n",
        "# رفع الموديل\n",
        "OUTPUT_DIR = \"./multilang_model\"\n",
        "upload_folder(\n",
        "    repo_id=REPO_ID,\n",
        "    folder_path=OUTPUT_DIR,\n",
        "    path_in_repo=\".\"\n",
        ")\n",
        "\n",
        "print(\"✅ Model uploaded successfully to:\", f\"https://huggingface.co/{REPO_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok-BXxO07ukI",
        "outputId": "b0067452-b3c3-46a0-a5f8-6b888e72c7e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `colab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `colab`\n",
            "Start hashing 7 files.\n",
            "Finished hashing 7 files.\n",
            "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
            "New Data Upload                         : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json:  98% 16.7M/17.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ...t/multilang_model/model.safetensors:   2% 16.8M/1.11G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json:  98% 16.7M/17.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 4)                :   3% 38.5M/1.13G [00:00<00:06, 170MB/s,   ???B/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 1.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :   6% 72.4M/1.13G [00:00<00:06, 171MB/s,  170MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 980kB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :   9% 106M/1.13G [00:00<00:06, 169MB/s,  169MB/s  ] \n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 654kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  12% 140M/1.13G [00:00<00:05, 169MB/s,  169MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 490kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  16% 182M/1.13G [00:01<00:05, 184MB/s,  179MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 392kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  19% 215M/1.13G [00:01<00:05, 178MB/s,  176MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:01<00:00, 327kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  23% 257M/1.13G [00:01<00:04, 188MB/s,  182MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:01<00:00, 280kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  26% 290M/1.13G [00:01<00:04, 182MB/s,  180MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:01<00:00, 245kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  29% 324M/1.13G [00:01<00:04, 177MB/s,  179MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:01<00:00, 218kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  32% 366M/1.13G [00:02<00:04, 187MB/s,  182MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:01<00:00, 196kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  35% 400M/1.13G [00:02<00:04, 181MB/s,  181MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:02<00:00, 178kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  38% 433M/1.13G [00:02<00:03, 177MB/s,  179MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:02<00:00, 163kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  42% 475M/1.13G [00:02<00:03, 187MB/s,  182MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:02<00:00, 151kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  45% 509M/1.13G [00:02<00:03, 181MB/s,  181MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:02<00:00, 140kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  49% 551M/1.13G [00:03<00:03, 190MB/s,  183MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:02<00:00, 130kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  51% 584M/1.13G [00:03<00:03, 183MB/s,  182MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:03<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:03<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:03<00:00, 122kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  54% 618M/1.13G [00:03<00:02, 179MB/s,  181MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:03<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:03<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:03<00:00, 115kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  58% 660M/1.13G [00:03<00:02, 188MB/s,  183MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:03<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:03<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:03<00:00, 109kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  61% 693M/1.13G [00:03<00:02, 182MB/s,  182MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:03<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:03<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:03<00:00, 103kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  64% 727M/1.13G [00:04<00:02, 177MB/s,  181MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:03<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:03<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:03<00:00, 97.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  66% 744M/1.13G [00:04<00:02, 150MB/s,  176MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:04<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:04<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:04<00:00, 93.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  68% 769M/1.13G [00:04<00:02, 142MB/s,  174MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:04<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:04<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:04<00:00, 89.0kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  70% 794M/1.13G [00:04<00:02, 138MB/s,  172MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:04<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:04<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:04<00:00, 85.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  71% 811M/1.13G [00:04<00:02, 121MB/s,  168MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:04<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:04<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:04<00:00, 81.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  74% 836M/1.13G [00:05<00:02, 123MB/s,  166MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:05<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:05<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:05<00:00, 78.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  76% 861M/1.13G [00:05<00:02, 123MB/s,  164MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:05<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:05<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:05<00:00, 75.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  77% 878M/1.13G [00:05<00:02, 112MB/s,  161MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:05<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:05<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:05<00:00, 72.5kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  80% 903M/1.13G [00:05<00:01, 116MB/s,  160MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:05<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:05<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:05<00:00, 69.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  81% 920M/1.13G [00:05<00:02, 106MB/s,  157MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:05<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:05<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:05<00:00, 67.5kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  83% 945M/1.13G [00:06<00:01, 112MB/s,  156MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:05<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:05<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:05<00:00, 65.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  86% 970M/1.13G [00:06<00:01, 116MB/s,  155MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:06<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:06<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:06<00:00, 63.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  87% 987M/1.13G [00:06<00:01, 107MB/s,  153MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:06<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:06<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:06<00:00, 61.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  89% 1.01G/1.13G [00:06<00:01, 112MB/s,  152MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:06<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:06<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:06<00:00, 59.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  91% 1.03G/1.13G [00:06<00:01, 104MB/s,  150MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:06<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:06<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:06<00:00, 57.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  93% 1.05G/1.13G [00:07<00:00, 110MB/s,  149MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:06<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:06<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:06<00:00, 55.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  94% 1.07G/1.13G [00:07<00:00, 102MB/s,  147MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:07<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:07<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:07<00:00, 54.4kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  97% 1.10G/1.13G [00:07<00:00, 109MB/s,  147MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:07<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:07<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:07<00:00, 52.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 4)                :  98% 1.11G/1.13G [00:07<00:00, 102MB/s,  145MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:07<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:07<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:07<00:00, 51.5kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (4 / 4)                : 100% 1.13G/1.13G [00:07<00:00, 104MB/s,  144MB/s  ]\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:07<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:07<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:07<00:00, 50.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ...t/multilang_model/model.safetensors: 100% 1.11G/1.11G [00:07<00:00, 140MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:07<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:07<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:07<00:00, 48.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ...t/multilang_model/model.safetensors: 100% 1.11G/1.11G [00:07<00:00, 137MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:08<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:08<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:08<00:00, 47.7kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ...t/multilang_model/model.safetensors: 100% 1.11G/1.11G [00:08<00:00, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:08<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:08<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:08<00:00, 47.7kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  ...t/multilang_model/model.safetensors: 100% 1.11G/1.11G [00:08<00:00, 133MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:08<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:08<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:08<00:00, 46.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (4 / 4)                : 100% 1.13G/1.13G [00:08<00:00, 132MB/s,  130MB/s  ]\n",
            "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...t/multilang_model/training_args.bin: 100% 5.71k/5.71k [00:08<?, ?B/s]\n",
            "  ...ilang_model/sentencepiece.bpe.model: 100% 5.07M/5.07M [00:08<?, ?B/s]\n",
            "  ...tent/multilang_model/tokenizer.json: 100% 17.1M/17.1M [00:08<00:00, 46.6kB/s]\n",
            "  ...t/multilang_model/model.safetensors: 100% 1.11G/1.11G [00:08<00:00, 130MB/s]\n",
            "Removing 7 file(s) from commit that have not changed.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "https://huggingface.co/amrhassank/Review_Classification/tree/main/.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"huggingface_hub[cli]\" --quiet\n",
        "!hf auth login\n",
        "!hf upload amrhassank/Review_Classification ./multilang_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dldSKL6iq340",
        "outputId": "e0d9fab1-220c-4d7d-a75d-3b6be0fb09ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.42.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.11.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.11.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.11.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install gradio transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8fk1KEhsmI1"
      },
      "outputs": [],
      "source": [
        "# import gradio as gr\n",
        "# import torch\n",
        "# from transformers import pipeline\n",
        "\n",
        "# # -----------------------------\n",
        "# # تحميل الموديل\n",
        "# # -----------------------------\n",
        "# MODEL_PATH = \"./multilang_model\"  # نفس مكان حفظ الموديل\n",
        "# device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# classifier = pipeline(\n",
        "#     \"text-classification\",\n",
        "#     model=MODEL_PATH,\n",
        "#     tokenizer=MODEL_PATH,\n",
        "#     device=device\n",
        "# )\n",
        "# label_map = {\n",
        "#     \"LABEL_0\": \"negative\",\n",
        "#     \"LABEL_1\": \"positive\",\n",
        "#     \"LABEL_2\": \"negative\",\n",
        "#     \"LABEL_3\": \"positive\"\n",
        "# }\n",
        "# # -----------------------------\n",
        "# # دالة التوقع\n",
        "# # -----------------------------\n",
        "# def predict_review(text):\n",
        "#     result = classifier(text)[0]\n",
        "#     label = result[\"label\"]\n",
        "#     score = result[\"score\"]\n",
        "#     mapped_label = label_map.get(label, \"unknown\")\n",
        "#     return f\"Predicted sentiment: {mapped_label}, confidence: {score:.4f}\"\n",
        "\n",
        "# # -----------------------------\n",
        "# # واجهة Gradio\n",
        "# # -----------------------------\n",
        "# interface = gr.Interface(\n",
        "#     fn=predict_review,\n",
        "#     inputs=gr.Textbox(lines=5, placeholder=\"اكتب مراجعة هنا بالعربي أو الإنجليزي...\"),\n",
        "#     outputs=\"text\",\n",
        "#     title=\"Multilingual Review Classifier\",\n",
        "#     description=\"ادخل مراجعة بالإنجليزي أو العربي، واحصل على توقع الإيجابي/السلبي للموديل.\"\n",
        "# )\n",
        "\n",
        "# # تشغيل الواجهة\n",
        "# interface.launch(share=True)  # share=True لو عايز رابط مباشر للويب\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "iezs4EVqHjKd",
        "outputId": "7ab2a405-2a50-46f9-e96b-afbd053c4919"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Deep_Models.ipynb'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-462141889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# افتح النوتبوك\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deep_Models.ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Deep_Models.ipynb'"
          ]
        }
      ],
      "source": [
        "import nbformat\n",
        "\n",
        "# افتح النوتبوك\n",
        "with open(\"Deep_Models.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# لو فيه metadata تالفة في widgets شيلها\n",
        "if \"widgets\" in nb[\"metadata\"]:\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "# احفظ نسخة نظيفة\n",
        "with open(\"Deep_Models_clean.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\"تم إنشاء Deep_Models_clean.ipynb بدون metadata تالفة ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQZK9wdKH98k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}